# -*- coding: utf-8 -*-
"""AIFinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rVABlnz5Sk68EWOGb4Mou_yP55cXbU6X

Importing necessary libraries
"""

import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
from transformers import BertTokenizerFast
from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, pipeline, AutoModelForSeq2SeqLM

"""Loading the pretrained Bert model for Named Entity Recognition (NER)"""

# Load pre-trained BERT model for NER
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")
# Create the NER pipeline
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

"""Load pre-trained BERT model for sentiment analysis"""

sentiment_tokenizer = AutoTokenizer.from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")
sentiment_model = AutoModelForSequenceClassification.from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")
# Create the sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model=sentiment_model, tokenizer=sentiment_tokenizer)

"""Function for web crawling"""

articles = {}
def get_links(link):

  return_links = []
  return_news = []
  r = requests.get(link)
  soup = BeautifulSoup(r.content, "html.parser")
  # soup = BeautifulSoup(r.content, "lxml")
  heading=soup.find('title')
  print(heading.get_text())

  if r.status_code != 200:
    print("Error. Something is wrong here")
  else:
    #Scraping news from MyJoyOnline
    if link == 'https://www.myjoyonline.com/':
      start = soup.findAll('div', class_='home-post-list-title')
      for news in start:
        a_tag = news.find('a')
        if a_tag:
            link = a_tag['href']
            # Find the h4 tag within the a tag to get the news text
            news_text = a_tag.find('h4').text
             # Add the news text and link to the dictionary
            articles[news_text] = {"link": link}

            # Fetch the content of the individual article page
            article_response = requests.get(link)
            article_soup = BeautifulSoup(article_response.content, 'html.parser')

            # Find the summary in the individual article page
            summary = article_soup.findAll('div', id="article-text", class_="mt-3 article-text")
            article_summary = ""
            for s in summary:
                p_tags = s.find_all('p')
                for p in p_tags:
                    article_summary += p.text.strip() + " "

            # Add the summary to the existing dictionary entry
            articles[news_text] = {"link": link, "summary": article_summary.strip()}

      # Print articles with summaries
      for news_text, data in articles.items():
          if 'summary' in data:
            print(f"Title: {news_text}")
            print(f"Link: {data['link']}")
            print(f"News: {data['summary']}")
            print("----")

    #Scraping news from Pulse Ghana
    elif link == "https://www.pulse.com.gh/":
      for links in soup.findAll('a', attrs={'href': re.compile("^http")}):
        news_text = links.get('title')
        return_news.append(news_text)
        link = links.get('href')
        return_links.append(link)
        articles[news_text] = {"link": link}

        # Fetch the content of the individual article page
        article_response = requests.get(link)
        article_soup = BeautifulSoup(article_response.content, 'html.parser')

        # Find the summary in the individual article page
        summary_start = article_soup.findAll('article')
        summary = article_soup.findAll('div', id="lead", class_="article-perex")
        article_summary = ""
        for s in summary:
            p_tags = s.find_all('p')
            for p in p_tags:
                article_summary += p.text.strip() + " "

        # Only add articles with summaries
        if article_summary != "":
            articles[news_text] = {"link": link, "summary": article_summary.strip()}
        else:
          del articles[news_text]

      # Print articles with summaries
      for news_text, data in articles.items():
          if 'summary' in data:
              print(f"Title: {news_text}")
              print(f"Link: {data['link']}")
              print(f"Summary: {data['summary']}")
              print("----")


    #Scraping news from Ghanaian Times
    elif link == "https://ghanaiantimes.com.gh/":
          start = soup.findAll('div', class_="post-details")
          next= soup.findAll('h2', class_="post-title")
          for links in next:
            a_tag = links.find('a')
            if a_tag:
                link = a_tag['href']
                # Find the h4 tag within the a tag to get the news text
                news_text = a_tag.find('h2')

                 # Add the news text and link to the dictionary
                articles[news_text] = {"link": link}

                # Fetch the content of the individual article page
                article_response = requests.get(link)
                article_soup = BeautifulSoup(article_response.content, 'html.parser')

                # Find the summary in the individual article page
                summary_start = article_soup.findAll('article', id="the-post", class_="container-wrapper post-content tie-standard")
                summary = article_soup.findAll('div', class_="entry-content entry clearfix is-expanded")
                article_summary = ""
                for s in summary:
                    p_tags = s.find_all('p')
                    print(p_tags)
                    for p in p_tags:
                        article_summary += p.text.strip() + " "

                    # Add the summary to the existing dictionary entry
                    articles[news_text]["summary"] = article_summary.strip()

            # Print articles with summaries
            for news_text, data in articles.items():
                if 'summary' in data:
                    print(f"Title: {news_text}")
                    print(f"Link: {data['link']}")
                    print(f"Summary: {data['summary']}")
                    print("----")


    #Scraping from Yen news
    elif link == "https://yen.com.gh/ghana/":
      start = soup.findAll('article', class_='c-article-card')
      for links in start:
            a_tag = links.find('a')
            if a_tag:
                link = a_tag['href']
                # Find the h4 tag within the a tag to get the news text
                news_text = a_tag.find('span').get_text()
                # Add the news text and link to the dictionary
                articles[news_text] = {"link": link}

                # Fetch the content of the individual article page
                article_response = requests.get("https://yen.com.gh/ghana/")
                article_soup = BeautifulSoup(article_response.content, 'html.parser')

                # Find the summary in the individual article page (adjust the selector)
                summary_elements = article_soup.find_all('div', class_='post__content')
                article_summary = ""
                for s in summary_elements:
                    article_summary += s.text.strip() + " "
                articles[news_text]['summary'] = article_summary.strip()

            # Print articles with summaries
            for news_text, data in articles.items():
                if 'summary' in data:
                    print(f"Title: {news_text}")
                    print(f"Link: {data['link']}")
                    print(f"Summary: {data['summary']}")
                    print("----")
                else:
                    print(f"Title: {news_text}")
                    print(f"Link: {data['link']}")
                    print("----")


    #Scraping news from GBC Ghana
    elif link == "https://www.gbcghanaonline.com/":
        start = soup.findAll('div', class_='elementor-post__text')
        for links in start:
            a_tag = links.find('a')
            if a_tag:
                link = a_tag['href']
                # Find the h4 tag within the a tag to get the news text
                news_text = a_tag.text.strip()
                # Add the news text and link to the dictionary
                articles[news_text] = {"link": link}

                # Fetch the content of the individual article page
                article_response = requests.get(link)
                article_soup = BeautifulSoup(article_response.content, 'html.parser')

                # Find the summary in the individual article page
                sum_start = article_soup.findAll('div', class_='e-con-inner')
                summary = article_soup.findAll('div', class_='elementor-widget-container')
                article_summary = ""
                for s in summary:
                    p_tags = s.find_all('p')
                    for p in p_tags:
                        article_summary += p.text.strip() + " "
                # Add the summary to the existing dictionary entry
                articles[news_text] = {"link": link, "summary": article_summary.strip()}

    # Print articles with summaries
    for news_text, data in articles.items():
        if 'summary' in data:
            print(f"Title: {news_text}")
            print(f"Link: {data['link']}")
            print(f"Summary: {data['summary']}")
            print("----")

  return articles

"""Loading the pretrained Bart model for text summarization"""

model_name = 'facebook/bart-large-cnn'
tokenizer = AutoTokenizer.from_pretrained(model_name)
summarizer = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""Function for text summarization"""

summarizer = pipeline('summarization', model=model_name, tokenizer= tokenizer)

def summarize_articles(articles):
    summarized_articles = {}
    for news_text, data in articles.items():
        if 'summary' in data:
            try:
                summarized_text = summarizer(data['summary'], max_length=125, min_length=25, do_sample=False)
                if summarized_text and len(summarized_text) > 0:
                    summarized_articles[news_text] = {
                        'link': data['link'],
                        'news': data['summary'],
                        'summary': summarized_text[0]['summary_text']
                    }
                else:
                    print(f"Summarizer returned an empty result for article: {news_text}")
            except Exception as e:
                print(f"Error summarizing article {news_text}: {str(e)}")
                summarized_articles[news_text] = {
                    'link': data['link'],
                    'news': data['summary'],
                    'summary': "Summary generation failed."
                }
    return summarized_articles

"""Applying the function to get news from various websites as well as summarizing the news"""

link = "https://www.pulse.com.gh/"
articles = get_links(link)
summarized_articles = summarize_articles(articles)

# Print summarized articles to verify
print(f"Number of summarized articles: {len(summarized_articles)}")
for title, content in summarized_articles.items():
    print(f"Title: {title}, Link: {content['link']}, Summary: {content.get('summary', 'No summary')}")

link = "https://www.myjoyonline.com/"
articles = get_links(link)
summarized_articles = summarize_articles(articles)

print(f"Number of summarized articles: {len(summarized_articles)}")
for title, content in summarized_articles.items():
    print(f"Title: {title}, Link: {content['link']}, Summary: {content.get('summary', 'No summary')}")

link = "https://yen.com.gh/ghana/"
articles = get_links(link)
summarized_articles = summarize_articles(articles)

link = "https://www.gbcghanaonline.com/"
articles = get_links(link)
summarized_articles = summarize_articles(articles)
summarized_articles

"""Storing all news articles in a dataframe"""

data = {
    "Title": list(articles.keys()),
    "Link": [articles[title]['link'] for title in articles.keys()],
    "Summary": [summarized_articles.get(title, {}).get('summary','') for title in articles.keys()]
}

articles = pd.DataFrame(data)
articles

"""Vectorizing the words in each article summary"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Vectorization
vectorizer = TfidfVectorizer(max_df=0.90, min_df=1, stop_words='english')
X = vectorizer.fit_transform(articles['Summary'])

# Check the shape of the matrix
print(X.shape)

"""Clustering the vectorized articles"""

from sklearn.cluster import DBSCAN
import numpy as np

# DBSCAN clustering
db = DBSCAN(eps=0.7, min_samples=3, metric='cosine').fit(X)

# Add cluster labels to the DataFrame
articles['Cluster'] = db.labels_

# Check the distribution of clusters
print(articles['Cluster'].value_counts())

"""Using cosine similarity to find which articles are relevant(THE HOTTEST! TOPIC IN TOWN)"""

# print(trending_articles_df.to_string())

from sklearn.feature_extraction.text import TfidfVectorizer

# Separate articles
yen_articles = articles[articles['Link'].str.contains('yen.com.gh')]
other_articles = articles[~articles['Link'].str.contains('yen.com.gh')]

# Fit vectorizer on all data to ensure consistent feature space
vectorizer = TfidfVectorizer(max_df=0.90, min_df=1, stop_words='english')
vectorizer.fit(articles['Title'].tolist() + articles['Summary'].tolist()) # Fit on both titles and summaries

# Transform titles and summaries separately using the fitted vectorizer
yen_vectors = vectorizer.transform(yen_articles['Title'])
other_vectors = vectorizer.transform(other_articles['Summary'])

# Combine vectors (adjust as needed)
all_vectors = np.concatenate((yen_vectors.toarray(), other_vectors.toarray()), axis=0)

# Check the shape of the matrix
print(all_vectors.shape)

from sklearn.cluster import DBSCAN
import numpy as np

# DBSCAN clustering
dbs = DBSCAN(eps=0.7, min_samples=3, metric='cosine').fit(all_vectors)

# Add cluster labels to the DataFrame
articles['Cluster'] = db.labels_

# Check the distribution of clusters
print(articles['Cluster'].value_counts())

def analyze_sentiment_and_cluster(articles_df, sentiment_pipeline, max_length=512): # Added max_length parameter
    # Perform sentiment analysis on all articles
    articles_df['Sentiment'] = articles_df['Summary'].apply(lambda x: sentiment_pipeline(x[:max_length])[0]['label'])

    # Filter articles that are in clusters other than -1
    filtered_df = articles_df[articles_df['Cluster'] != -1]


    for cluster_id in filtered_df['Cluster'].unique():
        cluster_articles = filtered_df[filtered_df['Cluster'] == cluster_id]
        print(f"Cluster ID: {cluster_id}")
        for idx, row in cluster_articles.iterrows():
            print(f"Title: {row['Title']}")
            print(f"Link: {row['Link']}")
            print(f"Summary: {row['Summary']}")
            print(f"Sentiment: {row['Sentiment']}")
            print("----")

# Perform sentiment analysis on articles
analyze_sentiment_and_cluster(articles, sentiment_pipeline)

import numpy as np
import torch
import torch.nn.functional as F
from transformers import pipeline

# Function to create a prediction with dropout
def predict_with_dropout(pipeline, text, dropout=0.5, num_iter=20):
    # Set the model in evaluation mode
    model = pipeline.model
    model.train()  # Enable dropout

    # List to store predictions
    predictions = []

    for _ in range(num_iter):
        with torch.no_grad():
            inputs = pipeline.tokenizer(text, return_tensors='pt', truncation=True, padding=True)
            outputs = model(**inputs)
            probs = F.softmax(outputs.logits, dim=-1).cpu().numpy()
            predictions.append(probs)

    return np.array(predictions)

# Example usage
texts = articles['Summary'].tolist()
dropout = 0.5
num_iter = 20

# Get predictions with dropout
predictions = np.array([predict_with_dropout(sentiment_pipeline, text, dropout, num_iter) for text in texts])

# Calculate the confidence intervals
ci = 0.8
lower_lim = np.quantile(predictions, 0.5-ci/2, axis=2)
upper_lim = np.quantile(predictions, 0.5+ci/2, axis=2)

# Print the confidence intervals for each text
for i, text in enumerate(texts):
    mean_prediction = predictions[i].mean(axis=0)
    lower_bound = lower_lim[i]
    upper_bound = upper_lim[i]

    print(f"Text: {text}")
    print(f"Prediction: {mean_prediction}")
    print(f"80% Confidence Interval: ({lower_bound}, {upper_bound})")
    print("----")

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping

num_classes = 10

# Fit and transform the data
train_data = vectorizer.fit_transform(articles).toarray()
train_data.shape

# Convert labels to NumPy array
train_labels = np.array(articles['Cluster'])

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=()),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model with early stopping
history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2, callbacks=[early_stopping])

